# Week 2: Observability & Monitoring for Agentic Systems ğŸ”

## Assignment Overview

**Mission**: Implement comprehensive monitoring and observability for agentic systems using Phoenix Arize and advanced evaluation techniques.

**Goal**: Transform our Week 1 CSV analytics agent from a "black box" into a fully observable, production-ready system with automated quality evaluation.

---

## ğŸ“ Project Structure

```
week_2/
â”œâ”€â”€ README.md                    # This file - project overview and implementation plan
â”œâ”€â”€ solution/
    â”œâ”€â”€ main.py                  # Advanced Week 2 application with enhanced tracing
    â”œâ”€â”€ monitoring/
        â”œâ”€â”€ enhanced_chat_service.py # Enhanced ChatService with custom Phoenix spans
        â”œâ”€â”€ tracing.py          # Advanced tracing utilities (custom spans)
        â”œâ”€â”€ evaluators.py       # LLM-as-a-Judge implementations
        â””â”€â”€ metrics.py          # Custom metrics and performance tracking
    â”œâ”€â”€ config/
        â””â”€â”€ phoenix_config.py   # Phoenix Arize setup and validation
    â””â”€â”€ examples/
        â”œâ”€â”€ trace_examples.py   # Sample traces for testing
        â””â”€â”€ evaluation_examples.py # LLM judge examples

week_1/solution/main.py          # Enhanced to optionally use Week 2 features
```

---

## âœ… Current Implementation Status

### What We HAVE âœ…

#### 1. Phoenix Arize Foundation (Partially Complete)
- âœ… **Phoenix dependency installed** - `arize-phoenix==11.21.0` in pyproject.toml
- âœ… **Basic tracing helper** - `get_tracing_provider()` function in helpers.py
- âœ… **Auto-instrumentation setup** - Basic OpenTelemetry integration in chat_service.py
- âœ… **Test framework** - test_phoenix_tracing.py validates basic connectivity
- âœ… **Environment variables** - PHOENIX_API_KEY and PHOENIX_ENDPOINT configured

#### 2. LLM-as-a-Judge Implementation (Advanced)
- âœ… **Complete evaluation framework** - tests/helpers/llm_judge.py
- âœ… **Evaluation criteria models** - Pydantic models for structured evaluation
- âœ… **Multiple evaluation types**:
  - Error response quality evaluation
  - Conversation flow evaluation  
  - User experience assessment
- âœ… **Judge agent implementation** - Specialized agent for response evaluation
- âœ… **Real-world usage** - Used in robustness testing (test_robustness_llm_judge.py)

#### 3. Working Agent System (Week 1)
- âœ… **Multi-agent CSV analytics** - Complete working system
- âœ… **Function tools** - Data analysis and visualization tools
- âœ… **Memory management** - SQLite sessions with conversation memory
- âœ… **Error handling** - Comprehensive guardrails and error recovery
- âœ… **Test coverage** - Unit, integration, and E2E tests

### What We NOW HAVE âœ… (Week 2 Implementation)

#### 1. Enhanced Phoenix Setup (IMPLEMENTED)
- âœ… **Advanced main application** - Week 2 main.py with comprehensive tracing
- âœ… **Enhanced ChatService** - Custom spans for business logic insights
- âœ… **Conversation flow grouping** - Hierarchical traces in Phoenix dashboard
- âœ… **Performance monitoring** - Custom spans for system operations
- âœ… **Session analytics** - Real-time usage pattern tracking

#### 2. Production-Ready Monitoring (IMPLEMENTED)
- âœ… **Message classification** - Business intelligence on query types
- âœ… **Session tracking** - Comprehensive conversation analytics
- âœ… **Error context** - Enhanced error tracing with business context
- âœ… **User experience metrics** - Response quality and interaction patterns

#### 3. Advanced Architecture (IMPLEMENTED)
- âœ… **Week 1 enhancement** - Optional Week 2 features in existing main
- âœ… **Modular design** - EnhancedChatService extends base functionality
- âœ… **Best practices** - Follows Phoenix documentation recommendations
- âœ… **Zero duplication** - Reuses existing Week 1 implementation

### What We STILL NEED âŒ (Future Enhancements)

#### 1. Advanced Evaluation Features (Bonus)
- âŒ **Hallucination detection** - Automated detection of incorrect agent responses
- âŒ **Relevance scoring** - Response quality and on-topic assessment
- âŒ **Trajectory evaluation** - Multi-turn conversation flow analysis
- âŒ **LLM-as-Judge integration** - Automated quality assessment pipeline

#### 2. Advanced Monitoring (Gold Level)
- âŒ **Cost tracking** - Token usage and API cost monitoring
- âŒ **Performance benchmarks** - Response time optimization
- âŒ **Tool usage analytics** - Function call success/failure patterns
- âŒ **Custom metrics** - Business-specific KPIs and dashboards

---

---

## ğŸ¯ Requirements Assessment

### âœ… Implemented
- **Phoenix Setup**: ArizeExportClient integration with API key authentication
- **Tracing**: Auto-instrumentation + custom conversation spans in Enhanced ChatService  
- **Dashboard**: Active Phoenix traces visible (conversation_feb95f3b_* spans with ChatCompletion children)
- **Analysis**: Complete trace reader framework with span filtering and data extraction
- **Performance Report**: [FINAL_REPORT.md](solution/FINAL_REPORT.md) - Production-ready Phoenix integration analysis

### âš ï¸ Current Limitations
- **Authentication**: Phoenix API returns PermissionDenied (likely needs PHOENIX_SPACE_ID)
- **Live Data**: Framework complete but can't pull traces due to auth issue
- **Evidence**: Dashboard shows real traces, API integration ready for proper credentials

### âŒ Missing Advanced Features
- **LLM-as-a-Judge**: No evaluators.py or automated quality evaluation (Silver level)
- **Custom Metrics**: No metrics.py or comprehensive token usage tracking (Gold level)

### Status: Bronze Level (Core) - Complete Framework with Auth Issue
**Meets**: Phoenix integration, tracing, visualization, performance analysis framework
**Evidence**: Real traces in dashboard, production-ready code
**Limitation**: API authentication needs PHOENIX_SPACE_ID for live data access

---

## ğŸ› ï¸ Development Environment

### Required Dependencies (Already Installed)
```toml
arize-phoenix = "11.21.0"
openinference-instrumentation-openai = "0.1.30" 
openinference-instrumentation-openai-agents = "1.2.0"
openai-agents = "0.2.6"
```

### Environment Variables Needed
```bash
PHOENIX_API_KEY=your_phoenix_api_key  # âœ… Configured in .env
PHOENIX_SPACE_ID=your_space_id        # âš ï¸ Needed for API access
PHOENIX_ENDPOINT=https://api.arize.com/phoenix
OPENAI_API_KEY=your_openai_key
```

### Quick Start Commands
```bash
# Set up environment
uv sync

# Run Phoenix connectivity test
uv run python tests/unit/test_phoenix_tracing.py

# Run existing CSV analytics system
cd week_1/solution && uv run python main.py

# Run comprehensive tests
uv run pytest tests/ -v
```



---

## ğŸš€ How to Use Week 2 Enhanced System

### Quick Start (Bronze Level Complete!)

1. **Run Week 2 Advanced Version**:
   ```bash
   cd week_2/solution
   python main.py
   ```

2. **Run Week 1 with Optional Enhancements**:
   ```bash
   cd week_1/solution
   python main.py  # Automatically detects and uses Week 2 features
   ```

3. **Compare Tracing**:
   - Week 1: Basic auto-instrumentation only
   - Week 2: Auto-instrumentation + custom conversation flow spans

### What You Get in Week 2 âœ¨

**Enhanced Phoenix Dashboard Experience**:
- ğŸ” **Conversation Flow Grouping**: Each user message creates a parent span containing all agent operations
- ğŸ“Š **Business Intelligence**: Message type classification and usage patterns
- ğŸ¯ **Session Analytics**: Real-time conversation statistics and insights
- ğŸ”§ **Advanced Context**: Rich metadata for debugging and optimization

**New Commands Available**:
- `trace` - Show current session trace context
- `analytics` - Detailed usage pattern analysis
- Regular quit commands show comprehensive session summary

## ğŸ“Š Performance Analysis

### Automated Report Generation

The system generates comprehensive performance reports by analyzing real Phoenix trace data:

**Data Source**: Phoenix dashboard traces from `analytics_system` project  
**Analysis Tool**: `phoenix_trace_reader.py` - Connects to Arize Phoenix API to pull trace data  
**Report Output**: [PERFORMANCE_REPORT.md](solution/PERFORMANCE_REPORT.md) - Automated analysis of agent behavior  

**How It Works**:
1. **Trace Collection**: `ArizeExportClient` pulls spans from Phoenix (conversation + ChatCompletion spans)
2. **Data Processing**: Extracts latency, costs, agent usage, conversation patterns from trace attributes
3. **Pattern Analysis**: Classifies interactions (analysis, visualization, data exploration) and agent performance
4. **Report Generation**: Automated insights on success rates, response times, cost efficiency, and recommendations

**Phoenix Dashboard Evidence** (from 8/25/2025, 05:25 PM):
- Multiple conversation spans (conversation_feb95f3b_1, _2, _3, etc.)
- ChatCompletion spans with timing: 2.33s, 4.84s, 1.78s, 6.74s, 0.96s, 1.45s, 2.18s
- Token usage: 648, 4,263, 3,819, 366, 3,931, 3,722, 452, 3,853, 3,559, 731 tokens
- Cost tracking: <$0.01 per call (realistic LLM API costs)

### Next Implementation Steps

1. **Test Enhanced Tracing**: Run sample conversations and check Phoenix dashboard
2. **Generate Reports**: Use `python monitoring/phoenix_trace_reader.py` (needs PHOENIX_SPACE_ID for live data)
3. **Implement LLM-as-a-Judge** (Silver Level): Add automated quality evaluation
4. **Add Cost Tracking** (Gold Level): Token usage and API cost monitoring

**Current Status**: Bronze Level âœ… Complete with enhanced conversation flow tracing and automated performance reporting!

Ready to see your agent conversations beautifully organized in Phoenix! ğŸ¯
